seed: 2024
sample_dir: './data/B'
output_dir: './checkpoints/B'
pretrained_model_path: 'stabilityai/stable-diffusion-2-1'


fine_style_training:
  resolution: 768
  placeholder: "<F>"
  init_token: "artistic"
  batch_size: 1

  textual_inversion:
    max_train_steps: 500
    learning_rate: 0.01
    adam_beta1: 0.85
    adam_beta2: 0.999
    adam_weight_decay: 1.0e-2
    adam_epsilon: 1.0e-8

  dreambooth_lora:
    max_train_steps: 500
    rank: 16
    learning_rate: 5.0e-4
    adam_beta1: 0.9
    adam_beta2: 0.999
    adam_weight_decay: 1.0e-2
    adam_epsilon: 1.0e-8


coarse_style_training:
  # dataset
  k: 15  # how many instances to generate for each example
  select_n: 10  # how many instances to be selected as training data from generated candidates. Lower than `k`.
  resolution: 768
  dir_to_save_dataset: './data/plausibleB'

  # CLIP model is solely used to assist data filtering during dataset creation process, not for training purposes.
  # It can be set it to null if we choose not to use the CLIP model for data creation.
  # Alternatively, we can manually filter a generated set of images, which may result in a higher quality dataset.
  # Note that using the CLIP model offers an automated and user-friendly approch for data creation.
  # However, in our experiments, we completed this process through manual selection. 
  # pretrained_clip_model: 'openai/clip-vit-large-patch14' 
  pretrained_clip_model: null

  # coarse style embeddings
  placeholder: "<C>"
  init_token: "&"

  # lora for text encoder
  lora_rank: 16

  # training
  max_train_steps: 300  
  batch_size: 1
  lr: 1.0e-4
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_weight_decay: 1.0e-2
  adam_epsilon: 1.0e-8
